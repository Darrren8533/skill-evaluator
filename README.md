# Skill Evaluator

A framework for evaluating, scanning, recommending, and generating [Claude Code](https://claude.ai/claude-code) skills (SKILL.md files).

## Features

| Command | Description |
|---|---|
| `evaluate` | Score a SKILL.md on 5 quality dimensions (0â€“100) |
| `security-scan` | Detect prompt injection, data exfiltration, backdoors |
| `recommend` | Recommend skills based on your tech stack & project type |
| `generate` | Generate a high-quality SKILL.md template from a topic |

## Installation

```bash
pip install -r requirements.txt
```

Requires a [Gemini API key](https://aistudio.google.com/app/apikey). Set it as an environment variable:

```bash
set GEMINI_API_KEY=your_key_here   # Windows
export GEMINI_API_KEY=your_key_here  # macOS/Linux
```

## Usage

### Evaluate a skill

```bash
python main.py evaluate path/to/SKILL.md
python main.py evaluate path/to/SKILL.md --json
python main.py evaluate path/to/SKILL.md --output report.txt
```

### Security scan

```bash
python main.py security-scan path/to/SKILL.md
```

Detects: prompt injection, sensitive file access, external data exfiltration, insecure coding advice, user deception.

### Recommend skills for your project

```bash
python main.py recommend --stack "Next.js, TypeScript, PostgreSQL" --type "SaaS Web App"
python main.py recommend -s "Python, FastAPI" -t "API Service" --show-skip
```

### Generate a skill template

```bash
python main.py generate --topic "API rate limiting best practices" --stack "Node.js, Express"
python main.py generate -t "Docker multi-stage builds" --evaluate
```

`--evaluate` runs the scorer immediately after generation so you can see the quality score.

## Scoring Dimensions

| Dimension | Weight | Description |
|---|---|---|
| Trigger Clarity | 20% | Clear when-to-use / when-not-to-use conditions |
| Structure Completeness | 25% | Has When to Use / Steps / Example / Expected Output |
| Step Executability | 25% | Concrete actions Claude can follow directly |
| Example Quality | 20% | Real Bad âŒ vs Good âœ… code comparisons |
| Scope Appropriateness | 10% | Focused topic, depth over breadth |

**Verdict:** `INSTALL` â‰¥ 75 Â· `MAYBE` 50â€“74 Â· `SKIP` < 50

> ğŸ’¡ A high score reflects documentation quality, not necessity. Generic skills (coding standards, Git conventions) are already known to Claude â€” install skills that enforce project-specific rules or team conventions.

## Security Risk Levels

`SAFE` â†’ `LOW` â†’ `MEDIUM` â†’ `HIGH` â†’ `CRITICAL`

**Recommendation:** `INSTALL` Â· `REVIEW` Â· `REJECT`

## Project Structure

```
skill-evaluator/
â”œâ”€â”€ main.py                      â† CLI entry point
â”œâ”€â”€ evaluator/
â”‚   â”œâ”€â”€ criteria.py              â† 5 scoring dimensions
â”‚   â”œâ”€â”€ scorer.py                â† Gemini-powered scorer
â”‚   â”œâ”€â”€ type_detector.py         â† self-contained vs index skill
â”‚   â”œâ”€â”€ report.py                â† text + JSON report output
â”‚   â””â”€â”€ security_scanner.py      â† regex + AI security scan
â”œâ”€â”€ recommender/
â”‚   â”œâ”€â”€ crawler.py               â† crawl GitHub skill repos
â”‚   â”œâ”€â”€ batch_evaluate.py        â† batch evaluate + cache
â”‚   â”œâ”€â”€ matcher.py               â† semantic relevance matching
â”‚   â””â”€â”€ ranker.py                â† 4-tier recommendation output
â”œâ”€â”€ generator/
â”‚   â””â”€â”€ template.py              â† SKILL.md template generator
â”œâ”€â”€ data/                        â† cached skills + results (gitignored)
â””â”€â”€ tests/sample_skills/
    â”œâ”€â”€ good_skill.md            â† 91/100 reference
    â”œâ”€â”€ bad_skill.md             â† 16/100 baseline
    â”œâ”€â”€ fake_good_skill.md       â† 37/100 adversarial test
    â””â”€â”€ malicious_skill.md       â† CRITICAL security test case
```

## Powered By

- [Google Gemini 2.0 Flash](https://ai.google.dev/) â€” scoring, security analysis, semantic matching, generation
- [Click](https://click.palletsprojects.com/) â€” CLI framework
